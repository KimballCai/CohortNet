{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6faf11e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:04:22.169481Z",
     "start_time": "2023-07-19T13:04:20.130168Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from util import *\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# define env setting\n",
    "sets = ['train', 'valid', 'test']\n",
    "\n",
    "from argument import get_parser\n",
    "from main import ModelTrainer\n",
    "\n",
    "from model import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca722ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:04:22.409112Z",
     "start_time": "2023-07-19T13:04:22.173570Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# this trainer is for the cohort modeling\n",
    "class CohortModelTrainer(ModelTrainer):\n",
    "    def __init__(self, args):\n",
    "        super(CohortModelTrainer, self).__init__(args)\n",
    "\n",
    "    def set_config(self):\n",
    "        # loss metrics\n",
    "        self.metrics = ['bceloss', 'auroc', 'auprc', 'accu', 'f1', 'minpse']\n",
    "        self.train_mode = \"CO\"\n",
    "\n",
    "    def learning(self):\n",
    "        for i_fold in self.args.folds:\n",
    "            logging.info('============= {}-th fold ============='.format(i_fold))\n",
    "            self.trainer_mode = \"train\"\n",
    "            self.dataset = {}\n",
    "            for name in sets:\n",
    "                self.dataset[name] = self.set_dataset(name, i_fold)\n",
    "            self.input_dim = self.dataset['train'].input_dim\n",
    "            self.output_dim = self.dataset['train'].output_dim\n",
    "            self.set_model(self.args)\n",
    "            self.recorders = Recorders(sets[1:], self.metrics, self.args.patience)\n",
    "\n",
    "            if self.args.fix:\n",
    "                self.args.cohort_iter = -1\n",
    "                self.fix_param()\n",
    "\n",
    "            if self.args.mode == \"train\":\n",
    "                self.criterion = self.set_criterion(type=self.args.criterion)\n",
    "                self.writer = SummaryWriter(self.log_path)\n",
    "\n",
    "                cohort_epoch = self.args.cohort_epoch\n",
    "                for epoch in range(1, self.args.epochs + 1):\n",
    "                    adjust_learning_rate(self.args, self.optimizer, epoch)\n",
    "\n",
    "                    if epoch > cohort_epoch:\n",
    "                        with torch.no_grad():\n",
    "                            self.cal_clusters(self.args, self.dataset['train'])\n",
    "\n",
    "                            if self.args.cohort_iter == -1:\n",
    "                                cohort_epoch = self.args.epochs + 1\n",
    "                            else:\n",
    "                                cohort_epoch += self.args.cohort_iter\n",
    "                        self.save_results(0, i_fold, None)\n",
    "\n",
    "                    time1 = time.time()\n",
    "                    loss = self.train(epoch)\n",
    "                    time2 = time.time()\n",
    "                    save_flag, results = self.validate(epoch)\n",
    "                    time3 = time.time()\n",
    "                    logging.info('Epoch {}, lr {:.6f}, train loss {:.4f}, '\n",
    "                                 'train time {:.2f}s, valid time {:.2f}s, total time {:.2f}s.'.format(\n",
    "                        epoch,\n",
    "                        self.optimizer.param_groups[0]['lr'],\n",
    "                        loss,\n",
    "                        time2 - time1,\n",
    "                        time3 - time2,\n",
    "                        time3 - time1\n",
    "                    ))\n",
    "                    logging.info(self.recorders.to_string())\n",
    "                    # save results\n",
    "                    for subset in range(len(self.recorders.sets)):\n",
    "                        for m in range(len(self.recorders.metrics)):\n",
    "                            self.writer.add_scalar(tag=\"%s/%s/%s\" % (self.train_mode,\n",
    "                                                                     self.recorders.sets[subset],\n",
    "                                                                     self.recorders.metrics[m]),\n",
    "                                                   scalar_value=results[subset, m], global_step=epoch)\n",
    "                    if save_flag == 1:\n",
    "                        self.save_results(epoch, i_fold, results)\n",
    "                    elif save_flag == 0:\n",
    "                        logging.info(\"[*] Overfitting... Stop!\")\n",
    "                        break\n",
    "                self.trainer_mode = \"eval\"\n",
    "                self.eval_model(os.path.join(self.log_path, 'ckpt_{i}.pth'.format(i=i_fold)))\n",
    "\n",
    "            elif self.args.mode == \"eval\":\n",
    "                self.trainer_mode = \"eval\"\n",
    "                assert args.model_path != \"#\", \"[x] Please provide a valid model path!\"\n",
    "                if not os.path.exists(args.model_path):\n",
    "                    logging.info(\"[x] Model path is invalid: %s\" % self.args.model_path)\n",
    "                self.eval_model(args.model_path)\n",
    "\n",
    "    def save_results(self, epoch, i_fold, results):\n",
    "        logging.info(\"[*] Saving files...\")\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if type(self.model) == torch.nn.DataParallel:\n",
    "            state['cohorts'] = self.model.module.cohorts\n",
    "        else:\n",
    "            state['cohorts'] = self.model.cohorts\n",
    "\n",
    "        save_file = os.path.join(self.log_path, 'ckpt_{i}.pth'.format(i=i_fold))\n",
    "        torch.save(state, save_file)\n",
    "        if epoch != 0:\n",
    "            np.save(os.path.join(self.log_path, \"best_valid_{i}\".format(i=i_fold)), results)\n",
    "            self.recorders.save(os.path.join(self.log_path, \"recorders_{i}.npz\".format(i=i_fold)))\n",
    "            self.recorders.record_to_csv(self.log_path)\n",
    "\n",
    "    def fix_param(self):\n",
    "        patterns = ['cohort']\n",
    "        logging.info(\"[*] non-fix pattern: \" + str(patterns))\n",
    "        fix_w_cnt = 0\n",
    "        fix_param_cnt = 0\n",
    "        for name, value in self.model.named_parameters():\n",
    "            value.requires_grad = False\n",
    "            fix_w_cnt += 1\n",
    "            fix_param_cnt += value.numel()\n",
    "            for p in patterns:\n",
    "                low_name = name.lower()\n",
    "                if p in low_name:\n",
    "                    value.requires_grad = True\n",
    "                    fix_w_cnt -= 1\n",
    "                    fix_param_cnt -= value.numel()\n",
    "                    continue\n",
    "        if self.args.debug:\n",
    "            for name, value in self.model.named_parameters():\n",
    "                logging.info(\"%s %s\"%(name, value.requires_grad))\n",
    "        logging.info(\"[*] No.weight is fixed: %d\" % fix_w_cnt)\n",
    "        logging.info(\"[*] No.params is fixed: %d\" % fix_param_cnt)\n",
    "        trainable_num = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        logging.info(\"[*] No.params is trainable: %d\" % trainable_num)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        ckpt = torch.load(model_path, map_location='cpu')\n",
    "        state_dict = ckpt['model']\n",
    "        model_dict = self.model.state_dict()\n",
    "        new_state_dict = {}\n",
    "        skipcount, loadedcount = 0, 0\n",
    "        for k, v in state_dict.items():\n",
    "            k2 = k.replace(\"module.\", \"\")\n",
    "            if (\"cohort\" in k.lower()) and self.trainer_mode == \"train\":\n",
    "                if self.args.debug:\n",
    "                    logging.info(\"%s is skipped\"%(k))\n",
    "                skipcount += 1\n",
    "                continue\n",
    "            if k in model_dict.keys():\n",
    "                new_state_dict[k] = v\n",
    "                loadedcount += 1\n",
    "            elif k2 in model_dict.keys():\n",
    "                new_state_dict[k2] = v\n",
    "                loadedcount += 1\n",
    "            else:\n",
    "                if self.args.debug:\n",
    "                    logging.info(\"%s is skipped\" % (k))\n",
    "                skipcount += 1\n",
    "        model_dict.update(new_state_dict)\n",
    "        self.model.load_state_dict(model_dict)\n",
    "\n",
    "        if \"cohorts\" in ckpt.keys():\n",
    "            logging.info(\"[*] Cohorts loaded.\")\n",
    "            if type(self.model) == torch.nn.DataParallel:\n",
    "                self.model.module.cohorts = ckpt['cohorts']\n",
    "            else:\n",
    "                self.model.cohorts = ckpt['cohorts']\n",
    "\n",
    "        logging.info(\"[*] Model loaded!\")\n",
    "        logging.info(\"[*] skipped: %d  loaded: %d\" % (skipcount, loadedcount))\n",
    "\n",
    "    def cal_clusters(self, args, dataset):\n",
    "        logging.info(\"[*] Start to calculate clusters. (totally GPU)\")\n",
    "        start = time.time()\n",
    "        self.model.eval()\n",
    "        attention = []\n",
    "\n",
    "        batch_size = 200\n",
    "        for batch_id, batch_x, batch_y in dataset.get_generator(batch_size, shuffle=False):\n",
    "            info, tdata, tmask, input_t = batch_x\n",
    "            tdata = torch.tensor(tdata).float()\n",
    "            tmask = torch.tensor(tmask).float()\n",
    "            if torch.cuda.is_available():\n",
    "                tdata = tdata.cuda()\n",
    "                tmask = tmask.cuda()\n",
    "            results = self.model([tdata, tmask])\n",
    "\n",
    "            if attention == []:\n",
    "                attention = results[2].detach().cpu()  # attention for interaction\n",
    "                t_rep = results[1].detach().cpu()  # B * T * F * H\n",
    "                pt_mask = tmask.detach().cpu()\n",
    "                label = batch_y\n",
    "            else:\n",
    "                attention = torch.cat((attention, results[2].detach().cpu()), 0)\n",
    "                t_rep = torch.cat((t_rep, results[1].detach().cpu()), 0)\n",
    "                pt_mask = torch.cat((pt_mask, tmask.detach().cpu()), 0)\n",
    "                label = np.concatenate((label, batch_y), 0)\n",
    "\n",
    "            # if batch_id == steps:\n",
    "            #     break\n",
    "        # cluster_in = torch.flatten(cluster_in, start_dim=2)\n",
    "        logging.info(\"[*] Get all representations.\")\n",
    "        logging.info(\"[*] attention shape: %s\" % str(attention.shape))\n",
    "        logging.info(\"[*] t rep shape: %s\" % str(t_rep.shape))\n",
    "\n",
    "        f_num = dataset.input_dim\n",
    "        time_dim = dataset.time_dim\n",
    "\n",
    "        pt_a = torch.flatten(attention, start_dim=0, end_dim=1)\n",
    "        pt_clusterin = torch.flatten(t_rep, start_dim=0, end_dim=1)  # B*T * F * Cp\n",
    "        pt_mask = pt_mask.any(1).byte().unsqueeze(1).repeat(1, time_dim, 1)  # B * T * F\n",
    "        pt_mask = torch.flatten(pt_mask, start_dim=0, end_dim=1)  # B*T * F\n",
    "        pt_rep = torch.flatten(t_rep, start_dim=0, end_dim=1)  # B*T * F * H\n",
    "\n",
    "        dup_label = np.tile(label[:, np.newaxis], (1, time_dim))\n",
    "        dup_label = torch.Tensor(dup_label.reshape(-1)).cuda()  # B*T\n",
    "\n",
    "        f_dim = t_rep.shape[-1]\n",
    "        num_clusters = np.ones(dataset.input_dim, dtype=np.int) * (args.k-1)\n",
    "        logging.info(\"[*] No. clusters: %s\"%str(num_clusters))\n",
    "        top_n_a = args.topn\n",
    "        min_freq = args.min_freq\n",
    "        min_sample_freq = args.min_sample_freq\n",
    "\n",
    "        f_centers = []\n",
    "        f_codes = None\n",
    "        for fid in range(f_num):\n",
    "            centers = None if self.model.module.cohorts == None else self.model.module.cohorts[fid]['centers'].cuda()\n",
    "            mask = pt_mask[:, fid].cuda()\n",
    "            sub_f_centers, sub_pos_f_codes = cluster(pt_clusterin[mask, fid, :].cuda(), num_clusters[fid], centers)\n",
    "            f_centers.append(sub_f_centers.detach())\n",
    "            sub_pos_f_codes = (sub_pos_f_codes + 2).detach()\n",
    "            mask_f_codes = torch.ones(pt_clusterin.shape[0], dtype=torch.long).cuda()\n",
    "            sub_f_codes = torch.zeros(pt_clusterin.shape[0], dtype=torch.long).cuda()\n",
    "            sub_f_codes = sub_f_codes.scatter_add_(0, torch.where(mask)[0], sub_pos_f_codes)\n",
    "            sub_f_codes = sub_f_codes.scatter_add_(0, torch.where(1 - mask)[0], mask_f_codes)\n",
    "            if fid == 0:\n",
    "                f_codes = sub_f_codes.unsqueeze(1)  # B*T * 1\n",
    "            else:\n",
    "                f_codes = torch.cat((f_codes, sub_f_codes.unsqueeze(1)), 1).detach()\n",
    "                # logging.info(torch.unique(f_codes))\n",
    "                # logging.info(\"[*] Get one feature state. %d\", fid)\n",
    "\n",
    "        logging.info(\"[*] Get all feature states.\")\n",
    "\n",
    "        # calculate the cohorts of features\n",
    "        cohorts = {}\n",
    "        cohort_size1, cohort_size2, cohort_size3, cohort_size4, cohort_size5 = [], [], [], [], []\n",
    "        cohort_freq, cohort_sam_freq = [], []\n",
    "        for fid in range(f_num):\n",
    "            process_start = time.time()\n",
    "            sub_pt = pt_a[:, fid, :].cuda()\n",
    "            f_a_index = torch.argsort(sub_pt)[:, -1 * top_n_a:]\n",
    "            f_a_top_mask = torch.eye(f_num)[f_a_index].sum(-2).cuda()\n",
    "            f_a_top_mask[:, fid] = 1\n",
    "            f_a_top_mask = f_a_top_mask.byte()\n",
    "            f_top = f_a_top_mask * f_codes  # B*T * F\n",
    "\n",
    "            logging.info(\"%d %s\"%(fid,f_top.shape))\n",
    "            if fid == 0: logging.info(\"remove the missing code for origin feature\")\n",
    "            # f_top_idx = torch.where(~(f_top == 1).any(1))[0]\n",
    "            f_top_idx = torch.where(f_top[:,fid] != 1)[0]\n",
    "            f_top = f_top[f_top_idx]\n",
    "            logging.info(\"%d %s\" % (fid, f_top.shape))\n",
    "\n",
    "            pattern, pat_index, pat_cnt = torch.unique(f_top, dim=0, return_counts=True, return_inverse=True)\n",
    "\n",
    "            # current version:  rep is based on pattern within codes\n",
    "            count_index = torch.argsort(pat_cnt)\n",
    "            pattern = pattern[count_index]\n",
    "            pat_cnt = pat_cnt[count_index]\n",
    "            # pat_index = pat_index[count_index]\n",
    "            cohort_size1.append(len(pattern))\n",
    "\n",
    "            match_id = torch.where(pat_cnt > min_freq)[0][0]\n",
    "            # filter1: minimal frequency\n",
    "            selected = match_id\n",
    "            pattern = pattern[selected:]  # C * F\n",
    "            cohort_size2.append(len(pattern))\n",
    "\n",
    "            # time-consuming when calculate cohorts\n",
    "            if fid == 0: logging.info(\"[*] use all patients with this pattern to learn cohorts\")\n",
    "            chunk_sample = 200\n",
    "            chunk_size = chunk_sample*time_dim  # 9600\n",
    "            num_records = int(f_codes.shape[0])\n",
    "            pat_chunk_size = 600\n",
    "            num_patterns = int(pattern.shape[0])\n",
    "            pat_cnt = torch.zeros(num_patterns, dtype=torch.float).cuda()\n",
    "            sample_pat_cnt = torch.zeros(num_patterns, dtype=torch.float).cuda()\n",
    "            pat_rep = torch.zeros(num_patterns, f_dim, dtype=torch.float).cuda()\n",
    "            pos_cnt = torch.zeros(num_patterns, dtype=torch.float).cuda()\n",
    "            neg_cnt = torch.zeros(num_patterns, dtype=torch.float).cuda()\n",
    "            pos_cnt_sample = torch.zeros(num_patterns, dtype=torch.float).cuda()\n",
    "            neg_cnt_sample = torch.zeros(num_patterns, dtype=torch.float).cuda()\n",
    "            pos_pat_rep = torch.zeros(num_patterns, f_dim, dtype=torch.float).cuda()\n",
    "            neg_pat_rep = torch.zeros(num_patterns, f_dim, dtype=torch.float).cuda()\n",
    "            for i in range(0, num_records, chunk_size):\n",
    "                begin = i\n",
    "                end = min(begin + chunk_size, num_records)\n",
    "                sub_tlabel = dup_label[begin:end]  # M*T\n",
    "                sub_f_codes = f_codes[begin:end, :]  # M * F\n",
    "                f_rep = pt_rep[begin:end, fid, :].cuda()  # M * H\n",
    "                for j in range(0, num_patterns, pat_chunk_size):\n",
    "                    pat_begin = j\n",
    "                    pat_end = min(pat_begin+pat_chunk_size, num_patterns)\n",
    "                    sub_pattern = pattern[pat_begin:pat_end]\n",
    "                    sub_pattern_mask = (sub_pattern != 0).byte()\n",
    "                    pattern_code = sub_f_codes.unsqueeze(1).byte() * sub_pattern_mask  # M * C * F\n",
    "                    pattern_match = (pattern_code == sub_pattern).all(2).byte()  # M * C\n",
    "                    subs_pat_match = pattern_match.reshape(int((end-begin)/48), time_dim, pat_end-pat_begin)  # S * T * C\n",
    "\n",
    "                    # general pattern rep\n",
    "                    sub_pat_rep = pattern_match.unsqueeze(-1) * f_rep.unsqueeze(1)  # M * C * H\n",
    "                    pat_rep[pat_begin:pat_end] += sub_pat_rep.sum(0)  # C * H\n",
    "                    pat_cnt[pat_begin:pat_end] += pattern_match.sum(0)  # C\n",
    "                    sample_pat_cnt[pat_begin:pat_end] += subs_pat_match.any(1).sum(0)  # C\n",
    "\n",
    "                    # pos and neg sample pattern rep\n",
    "                    pos_pat_match = sub_tlabel.unsqueeze(1) * pattern_match  # M * C\n",
    "                    neg_pat_match = (1-sub_tlabel.unsqueeze(1)) * pattern_match  # M * C\n",
    "                    pos_pat_rep[pat_begin:pat_end] += (pos_pat_match.unsqueeze(2) * f_rep.unsqueeze(1)).sum(0)\n",
    "                    neg_pat_rep[pat_begin:pat_end] += (neg_pat_match.unsqueeze(2) * f_rep.unsqueeze(1)).sum(0)\n",
    "                    pos_cnt[pat_begin:pat_end] += pos_pat_match.sum(0)\n",
    "                    neg_cnt[pat_begin:pat_end] += neg_pat_match.sum(0)\n",
    "                    pos_pat_match_sample = pos_pat_match.reshape(-1, time_dim, pat_end - pat_begin).any(1)  # S * C\n",
    "                    pos_cnt_sample[pat_begin:pat_end] += pos_pat_match_sample.sum(0)  # C\n",
    "                    neg_pat_match_sample = neg_pat_match.reshape(-1, time_dim, pat_end - pat_begin).any(1)  # S * C\n",
    "                    neg_cnt_sample[pat_begin:pat_end] += neg_pat_match_sample.sum(0)  # C\n",
    "\n",
    "            pat_rep = pat_rep/pat_cnt.unsqueeze(1)  # C * H\n",
    "            pos_pat_rep = pos_pat_rep/(pos_cnt+10e-6).unsqueeze(1)  # C * H\n",
    "            neg_pat_rep = neg_pat_rep/(neg_cnt+10e-6).unsqueeze(1)  # C * H\n",
    "\n",
    "            # filter2: pattern exists on more than x samples\n",
    "            filter2_idx = torch.where(sample_pat_cnt >= min_sample_freq)[0]\n",
    "            pattern = pattern[filter2_idx]\n",
    "            pat_cnt = pat_cnt[filter2_idx]\n",
    "            sample_pat_cnt = sample_pat_cnt[filter2_idx]\n",
    "            pat_rep = pat_rep[filter2_idx]\n",
    "            pos_cnt = pos_cnt[filter2_idx]\n",
    "            pos_pat_rep = pos_pat_rep[filter2_idx]\n",
    "            pos_cnt_sample = pos_cnt_sample[filter2_idx]\n",
    "            neg_cnt = neg_cnt[filter2_idx]\n",
    "            neg_pat_rep = neg_pat_rep[filter2_idx]\n",
    "            neg_cnt_sample = neg_cnt_sample[filter2_idx]\n",
    "            cohort_size3.append(len(filter2_idx))\n",
    "\n",
    "            # sorted by No.samples\n",
    "            sample_cnt_idx = torch.argsort(sample_pat_cnt)\n",
    "            pattern = pattern[sample_cnt_idx]\n",
    "            pat_cnt = pat_cnt[sample_cnt_idx]\n",
    "            sample_pat_cnt = sample_pat_cnt[sample_cnt_idx]\n",
    "            pat_rep = pat_rep[sample_cnt_idx]\n",
    "            pos_cnt = pos_cnt[sample_cnt_idx]\n",
    "            pos_pat_rep = pos_pat_rep[sample_cnt_idx]\n",
    "            neg_cnt = neg_cnt[sample_cnt_idx]\n",
    "            neg_pat_rep = neg_pat_rep[sample_cnt_idx]\n",
    "            pos_cnt_sample = pos_cnt_sample[sample_cnt_idx]\n",
    "            neg_cnt_sample = neg_cnt_sample[sample_cnt_idx]\n",
    "\n",
    "            if len(pat_cnt) == 0:\n",
    "                cohort_freq.append(0)\n",
    "                cohort_sam_freq.append(0)\n",
    "            else:\n",
    "                cohort_freq.append(min(pat_cnt).cpu().numpy())\n",
    "                cohort_sam_freq.append(min(sample_pat_cnt).cpu().numpy())\n",
    "\n",
    "            process_end = time.time()\n",
    "            logging.info(\"[*] Process Feature %d with %.3f seconds\" % (fid, process_end-process_start))\n",
    "            cohorts[fid] = {\n",
    "                \"pat\": pattern.cpu().detach(),\n",
    "                \"pat_count\": pat_cnt.cpu().detach(),\n",
    "                \"sample_pat_cnt\": sample_pat_cnt.cpu().detach(),\n",
    "                \"pat_rep\": pat_rep.cpu().detach(),\n",
    "                \"labeled\": {\n",
    "                    \"pos_cnt\": pos_cnt.cpu().detach(),\n",
    "                    \"pos_rep\": pos_pat_rep.cpu().detach(),\n",
    "                    \"neg_cnt\": neg_cnt.cpu().detach(),\n",
    "                    \"neg_rep\": neg_pat_rep.cpu().detach(),\n",
    "                    \"pos_cnt_sample\": pos_cnt_sample.cpu().detach(),\n",
    "                    \"neg_cnt_sample\": neg_cnt_sample.cpu().detach(),\n",
    "                },\n",
    "                \"centers\": f_centers[fid].cpu().detach(),\n",
    "            }\n",
    "\n",
    "        end = time.time()\n",
    "        logging.info(\"[*] No.cohort in total\")\n",
    "        logging.info(np.array(cohort_size1))\n",
    "        logging.info(\"[*] Filter1: No.cohort filtered by org freq\")\n",
    "        logging.info(np.array(cohort_size2))\n",
    "        logging.info(\"[*] Filter2: No.cohort filtered by sample freq\")\n",
    "        logging.info(np.array(cohort_size3))\n",
    "        logging.info(\"[*] Filter3: No.cohort filtered by max size\")\n",
    "        logging.info(np.array(cohort_size5))\n",
    "        logging.info(\"[*] minimal cohort freq\")\n",
    "        logging.info(np.array(cohort_freq))\n",
    "        logging.info(\"[*] minimal cohort freq (sample-level)\")\n",
    "        logging.info(np.array(cohort_sam_freq))\n",
    "        logging.info(\"[*] Build all feature clusters: total time {:.2f}s \".format(end - start))\n",
    "        self.model.module.cohorts = cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826e4268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T13:04:22.448090Z",
     "start_time": "2023-07-19T13:04:22.412125Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(active='relu', application='inhos_mortality', batch_size=128, clip_max=3.0, clip_min=-3.0, cohort_epoch=0, cohort_iter=-1, compress_dim=24, cosine=True, criterion='bce', data_clip=False, data_clip_max=inf, data_clip_min=-inf, dataset='MIMIC3', dataset_mode='regular', debug=False, embed_dim=24, epochs=80, ffill=True, ffill_steps=48, fix=True, folds=[3], fusion_dim=32, gpu='3,4,5', hidden_dim=32, inter_type='mul', k=7, lr=0.0005, lr_decay_epochs='150,350,500', lr_decay_rate=0.1, max_cohort_size=8000, max_timesteps=48, min_freq=10, min_sample_freq=5, mode='train', model='CohortNet', model_path='/???????/inhos_mortality/CohortNet_SP/CohortNet_20230718071054_random/ckpt_3.pth', momentum=0.9, opt='adam', patience=10, print_freq=20, random=True, standardization=True, topn=3, warm=False, weight_decay=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from argument import base_train_args, regular_dataset, CohortNet_parse_args, Cohort_args\n",
    "import argparse\n",
    "base_parser = argparse.ArgumentParser(description=\"ECHO Framework.\",add_help=False, \n",
    "                                      parents=[base_train_args(), regular_dataset(), \n",
    "                                               CohortNet_parse_args(), Cohort_args()])\n",
    "args = base_parser.parse_args([])\n",
    "args.warm=False\n",
    "args.fix=True\n",
    "args.model_path=\"please config the path\"\n",
    "args.lr=5e-4\n",
    "args.topn=3\n",
    "args.k=7\n",
    "args.epochs=80\n",
    "args.gpu=\"3,4,5\"\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2f2277",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T16:57:53.636661Z",
     "start_time": "2023-07-19T13:04:22.450256Z"
    },
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-19-23 13:04:================== Start 20230719130422 ==================\n",
      "07-19-23 13:04:[*]random seed: 34974\n",
      "07-19-23 13:04:Timestamp: 20230719130422\n",
      "07-19-23 13:04:Arguments\n",
      "07-19-23 13:04:active = relu\n",
      "07-19-23 13:04:application = inhos_mortality\n",
      "07-19-23 13:04:batch_size = 128\n",
      "07-19-23 13:04:clip_max = 3.0\n",
      "07-19-23 13:04:clip_min = -3.0\n",
      "07-19-23 13:04:cohort_epoch = 0\n",
      "07-19-23 13:04:cohort_iter = -1\n",
      "07-19-23 13:04:compress_dim = 24\n",
      "07-19-23 13:04:cosine = True\n",
      "07-19-23 13:04:criterion = bce\n",
      "07-19-23 13:04:data_clip = False\n",
      "07-19-23 13:04:data_clip_max = inf\n",
      "07-19-23 13:04:data_clip_min = -inf\n",
      "07-19-23 13:04:dataset = MIMIC3\n",
      "07-19-23 13:04:dataset_mode = regular\n",
      "07-19-23 13:04:debug = False\n",
      "07-19-23 13:04:embed_dim = 24\n",
      "07-19-23 13:04:epochs = 80\n",
      "07-19-23 13:04:ffill = True\n",
      "07-19-23 13:04:ffill_steps = 48\n",
      "07-19-23 13:04:fix = True\n",
      "07-19-23 13:04:folds = [3]\n",
      "07-19-23 13:04:fusion_dim = 32\n",
      "07-19-23 13:04:gpu = 3,4,5\n",
      "07-19-23 13:04:hidden_dim = 32\n",
      "07-19-23 13:04:inter_type = mul\n",
      "07-19-23 13:04:k = 7\n",
      "07-19-23 13:04:lr = 0.0005\n",
      "07-19-23 13:04:lr_decay_epochs = 150,350,500\n",
      "07-19-23 13:04:lr_decay_rate = 0.1\n",
      "07-19-23 13:04:max_cohort_size = 8000\n",
      "07-19-23 13:04:max_timesteps = 48\n",
      "07-19-23 13:04:min_freq = 10\n",
      "07-19-23 13:04:min_sample_freq = 5\n",
      "07-19-23 13:04:mode = train\n",
      "07-19-23 13:04:model = CohortNet\n",
      "07-19-23 13:04:model_path = /?????????/inhos_mortality/CohortNet_SP/CohortNet_20230718071054_random/ckpt_3.pth\n",
      "07-19-23 13:04:momentum = 0.9\n",
      "07-19-23 13:04:opt = adam\n",
      "07-19-23 13:04:patience = 10\n",
      "07-19-23 13:04:print_freq = 20\n",
      "07-19-23 13:04:random = True\n",
      "07-19-23 13:04:standardization = True\n",
      "07-19-23 13:04:topn = 3\n",
      "07-19-23 13:04:warm = False\n",
      "07-19-23 13:04:weight_decay = 0\n",
      "07-19-23 13:04:============= 3-th fold =============\n",
      "07-19-23 13:04:[*]loading the MIMIC3 dataset: train.\n",
      "07-19-23 13:04:[*] train: ['85929_episode1.npz' '90115_episode1.npz' '98016_episode1.npz'\n",
      " '45395_episode2.npz' '12250_episode1.npz']\n",
      "100%|██████████| 16911/16911 [01:02<00:00, 270.19it/s]\n",
      "07-19-23 13:05:[*]loading the MIMIC3 dataset: valid.\n",
      "07-19-23 13:05:[*] valid: ['27309_episode1.npz' '88214_episode1.npz' '89562_episode1.npz'\n",
      " '28578_episode1.npz' '47918_episode1.npz']\n",
      "100%|██████████| 2114/2114 [00:07<00:00, 291.42it/s]\n",
      "07-19-23 13:05:[*]loading the MIMIC3 dataset: test.\n",
      "07-19-23 13:05:[*] test: ['27586_episode1.npz' '43472_episode2.npz' '16776_episode1.npz'\n",
      " '5589_episode1.npz' '11056_episode1.npz']\n",
      "100%|██████████| 2114/2114 [00:07<00:00, 291.33it/s]\n",
      "07-19-23 13:05:[*] optimizer: Adam, lr: 0.000500, weight decay: 0.000000\n",
      "07-19-23 13:05:DataParallel(\n",
      "  (module): CohortNet(\n",
      "    (MFLM): MultiChannelFeatureLearningModule(\n",
      "      (biEmbedding): BiEmbedding()\n",
      "      (featureTrend): FeatureTrendLearning(\n",
      "        (embedRnns): ModuleList(\n",
      "          (0): GRU(24, 24, batch_first=True)\n",
      "          (1): GRU(24, 24, batch_first=True)\n",
      "          (2): GRU(24, 24, batch_first=True)\n",
      "          (3): GRU(24, 24, batch_first=True)\n",
      "          (4): GRU(24, 24, batch_first=True)\n",
      "          (5): GRU(24, 24, batch_first=True)\n",
      "          (6): GRU(24, 24, batch_first=True)\n",
      "          (7): GRU(24, 24, batch_first=True)\n",
      "          (8): GRU(24, 24, batch_first=True)\n",
      "          (9): GRU(24, 24, batch_first=True)\n",
      "          (10): GRU(24, 24, batch_first=True)\n",
      "          (11): GRU(24, 24, batch_first=True)\n",
      "          (12): GRU(24, 24, batch_first=True)\n",
      "          (13): GRU(24, 24, batch_first=True)\n",
      "          (14): GRU(24, 24, batch_first=True)\n",
      "          (15): GRU(24, 24, batch_first=True)\n",
      "          (16): GRU(24, 24, batch_first=True)\n",
      "          (17): GRU(24, 24, batch_first=True)\n",
      "          (18): GRU(24, 24, batch_first=True)\n",
      "          (19): GRU(24, 24, batch_first=True)\n",
      "          (20): GRU(24, 24, batch_first=True)\n",
      "          (21): GRU(24, 24, batch_first=True)\n",
      "          (22): GRU(24, 24, batch_first=True)\n",
      "          (23): GRU(24, 24, batch_first=True)\n",
      "          (24): GRU(24, 24, batch_first=True)\n",
      "          (25): GRU(24, 24, batch_first=True)\n",
      "          (26): GRU(24, 24, batch_first=True)\n",
      "          (27): GRU(24, 24, batch_first=True)\n",
      "          (28): GRU(24, 24, batch_first=True)\n",
      "          (29): GRU(24, 24, batch_first=True)\n",
      "          (30): GRU(24, 24, batch_first=True)\n",
      "          (31): GRU(24, 24, batch_first=True)\n",
      "          (32): GRU(24, 24, batch_first=True)\n",
      "          (33): GRU(24, 24, batch_first=True)\n",
      "          (34): GRU(24, 24, batch_first=True)\n",
      "          (35): GRU(24, 24, batch_first=True)\n",
      "          (36): GRU(24, 24, batch_first=True)\n",
      "          (37): GRU(24, 24, batch_first=True)\n",
      "          (38): GRU(24, 24, batch_first=True)\n",
      "          (39): GRU(24, 24, batch_first=True)\n",
      "          (40): GRU(24, 24, batch_first=True)\n",
      "          (41): GRU(24, 24, batch_first=True)\n",
      "          (42): GRU(24, 24, batch_first=True)\n",
      "          (43): GRU(24, 24, batch_first=True)\n",
      "          (44): GRU(24, 24, batch_first=True)\n",
      "          (45): GRU(24, 24, batch_first=True)\n",
      "          (46): GRU(24, 24, batch_first=True)\n",
      "          (47): GRU(24, 24, batch_first=True)\n",
      "          (48): GRU(24, 24, batch_first=True)\n",
      "          (49): GRU(24, 24, batch_first=True)\n",
      "          (50): GRU(24, 24, batch_first=True)\n",
      "          (51): GRU(24, 24, batch_first=True)\n",
      "          (52): GRU(24, 24, batch_first=True)\n",
      "          (53): GRU(24, 24, batch_first=True)\n",
      "          (54): GRU(24, 24, batch_first=True)\n",
      "          (55): GRU(24, 24, batch_first=True)\n",
      "          (56): GRU(24, 24, batch_first=True)\n",
      "          (57): GRU(24, 24, batch_first=True)\n",
      "          (58): GRU(24, 24, batch_first=True)\n",
      "          (59): GRU(24, 24, batch_first=True)\n",
      "          (60): GRU(24, 24, batch_first=True)\n",
      "          (61): GRU(24, 24, batch_first=True)\n",
      "          (62): GRU(24, 24, batch_first=True)\n",
      "        )\n",
      "      )\n",
      "      (featureInteractionModule): FeatureInteractionLearning(\n",
      "        (a_ff): ModuleList(\n",
      "          (0): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (1): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (2): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (3): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (4): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (5): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (6): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (7): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (8): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (9): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (10): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (11): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (12): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (13): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (14): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (15): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (16): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (17): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (18): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (19): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (20): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (21): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (22): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (23): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (24): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (25): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (26): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (27): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (28): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (29): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (30): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (31): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (32): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (33): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (34): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (35): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (36): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (37): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (38): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (39): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (40): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (41): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (42): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (43): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (44): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (45): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (46): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (47): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (48): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (49): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (50): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (51): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (52): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (53): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (54): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (55): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (56): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (57): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (58): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (59): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (60): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (61): Linear(in_features=24, out_features=24, bias=True)\n",
      "          (62): Linear(in_features=24, out_features=24, bias=True)\n",
      "        )\n",
      "        (a_ff2): ModuleList(\n",
      "          (0): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (1): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (2): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (3): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (4): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (5): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (6): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (7): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (8): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (9): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (10): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (11): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (12): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (13): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (14): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (15): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (16): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (17): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (18): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (19): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (20): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (21): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (22): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (23): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (24): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (25): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (26): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (27): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (28): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (29): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (30): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (31): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (32): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (33): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (34): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (35): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (36): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (37): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (38): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (39): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (40): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (41): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (42): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (43): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (44): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (45): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (46): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (47): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (48): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (49): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (50): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (51): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (52): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (53): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (54): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (55): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (56): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (57): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (58): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (59): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (60): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (61): Linear(in_features=24, out_features=1, bias=False)\n",
      "          (62): Linear(in_features=24, out_features=1, bias=False)\n",
      "        )\n",
      "        (active): ReLU(inplace=True)\n",
      "        (softmax): Softmax(dim=2)\n",
      "      )\n",
      "      (featureFusionModule): FeatureFusion(\n",
      "        (active): ReLU()\n",
      "        (compressor): Linear(in_features=72, out_features=32, bias=True)\n",
      "      )\n",
      "      (rnns): ModuleList(\n",
      "        (0): GRU(32, 32, batch_first=True)\n",
      "        (1): GRU(32, 32, batch_first=True)\n",
      "        (2): GRU(32, 32, batch_first=True)\n",
      "        (3): GRU(32, 32, batch_first=True)\n",
      "        (4): GRU(32, 32, batch_first=True)\n",
      "        (5): GRU(32, 32, batch_first=True)\n",
      "        (6): GRU(32, 32, batch_first=True)\n",
      "        (7): GRU(32, 32, batch_first=True)\n",
      "        (8): GRU(32, 32, batch_first=True)\n",
      "        (9): GRU(32, 32, batch_first=True)\n",
      "        (10): GRU(32, 32, batch_first=True)\n",
      "        (11): GRU(32, 32, batch_first=True)\n",
      "        (12): GRU(32, 32, batch_first=True)\n",
      "        (13): GRU(32, 32, batch_first=True)\n",
      "        (14): GRU(32, 32, batch_first=True)\n",
      "        (15): GRU(32, 32, batch_first=True)\n",
      "        (16): GRU(32, 32, batch_first=True)\n",
      "        (17): GRU(32, 32, batch_first=True)\n",
      "        (18): GRU(32, 32, batch_first=True)\n",
      "        (19): GRU(32, 32, batch_first=True)\n",
      "        (20): GRU(32, 32, batch_first=True)\n",
      "        (21): GRU(32, 32, batch_first=True)\n",
      "        (22): GRU(32, 32, batch_first=True)\n",
      "        (23): GRU(32, 32, batch_first=True)\n",
      "        (24): GRU(32, 32, batch_first=True)\n",
      "        (25): GRU(32, 32, batch_first=True)\n",
      "        (26): GRU(32, 32, batch_first=True)\n",
      "        (27): GRU(32, 32, batch_first=True)\n",
      "        (28): GRU(32, 32, batch_first=True)\n",
      "        (29): GRU(32, 32, batch_first=True)\n",
      "        (30): GRU(32, 32, batch_first=True)\n",
      "        (31): GRU(32, 32, batch_first=True)\n",
      "        (32): GRU(32, 32, batch_first=True)\n",
      "        (33): GRU(32, 32, batch_first=True)\n",
      "        (34): GRU(32, 32, batch_first=True)\n",
      "        (35): GRU(32, 32, batch_first=True)\n",
      "        (36): GRU(32, 32, batch_first=True)\n",
      "        (37): GRU(32, 32, batch_first=True)\n",
      "        (38): GRU(32, 32, batch_first=True)\n",
      "        (39): GRU(32, 32, batch_first=True)\n",
      "        (40): GRU(32, 32, batch_first=True)\n",
      "        (41): GRU(32, 32, batch_first=True)\n",
      "        (42): GRU(32, 32, batch_first=True)\n",
      "        (43): GRU(32, 32, batch_first=True)\n",
      "        (44): GRU(32, 32, batch_first=True)\n",
      "        (45): GRU(32, 32, batch_first=True)\n",
      "        (46): GRU(32, 32, batch_first=True)\n",
      "        (47): GRU(32, 32, batch_first=True)\n",
      "        (48): GRU(32, 32, batch_first=True)\n",
      "        (49): GRU(32, 32, batch_first=True)\n",
      "        (50): GRU(32, 32, batch_first=True)\n",
      "        (51): GRU(32, 32, batch_first=True)\n",
      "        (52): GRU(32, 32, batch_first=True)\n",
      "        (53): GRU(32, 32, batch_first=True)\n",
      "        (54): GRU(32, 32, batch_first=True)\n",
      "        (55): GRU(32, 32, batch_first=True)\n",
      "        (56): GRU(32, 32, batch_first=True)\n",
      "        (57): GRU(32, 32, batch_first=True)\n",
      "        (58): GRU(32, 32, batch_first=True)\n",
      "        (59): GRU(32, 32, batch_first=True)\n",
      "        (60): GRU(32, 32, batch_first=True)\n",
      "        (61): GRU(32, 32, batch_first=True)\n",
      "        (62): GRU(32, 32, batch_first=True)\n",
      "      )\n",
      "      (active): ReLU()\n",
      "      (linears): ModuleList(\n",
      "        (0): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (1): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (2): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (3): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (4): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (5): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (6): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (7): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (8): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (9): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (10): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (11): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (12): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (13): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (14): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (15): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (16): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (17): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (18): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (19): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (20): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (21): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (22): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (23): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (24): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (25): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (26): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (27): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (28): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (29): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (30): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (31): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (32): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (33): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (34): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (35): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (36): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (37): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (38): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (39): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (40): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (41): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (42): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (43): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (44): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (45): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (46): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (47): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (48): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (49): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (50): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (51): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (52): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (53): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (54): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (55): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (56): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (57): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (58): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (59): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (60): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (61): Linear(in_features=32, out_features=24, bias=True)\n",
      "        (62): Linear(in_features=32, out_features=24, bias=True)\n",
      "      )\n",
      "      (prediction): Linear(in_features=1512, out_features=1, bias=True)\n",
      "    )\n",
      "    (CEM): CohortExploitationModule(\n",
      "      (active): ReLU()\n",
      "      (cohort_attention): ModuleList(\n",
      "        (0): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (1): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (2): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (3): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (4): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (5): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (6): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (7): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (8): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (9): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (10): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (11): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (12): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (13): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (14): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (15): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (16): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (17): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (18): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (19): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (20): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (21): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (22): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (23): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (24): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (25): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (26): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (27): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (28): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (29): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (30): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (31): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (32): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (33): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (34): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (35): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (36): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (37): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (38): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (39): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (40): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (41): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (42): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (43): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (44): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (45): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (46): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (47): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (48): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (49): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (50): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (51): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (52): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (53): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (54): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (55): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (56): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (57): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (58): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (59): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (60): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (61): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "        (62): FinalAttentionQKV(\n",
      "          (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (W_k): Linear(in_features=33, out_features=32, bias=True)\n",
      "          (W_v): Linear(in_features=33, out_features=33, bias=True)\n",
      "          (tanh): Tanh()\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (sigmoid): Sigmoid()\n",
      "        )\n",
      "      )\n",
      "      (compress): Linear(in_features=2016, out_features=32, bias=True)\n",
      "      (predictionCohort): Linear(in_features=2079, out_features=1, bias=False)\n",
      "    )\n",
      "    (activation): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-19-23 13:05:Model param num: 995942  trainable: 995942\n",
      "07-19-23 13:05:[*] Model loaded!\n",
      "07-19-23 13:05:[*] skipped: 379  loaded: 828\n",
      "07-19-23 13:05:[*] non-fix pattern: ['cohort']\n",
      "07-19-23 13:05:[*] No.weight is fixed: 828\n",
      "07-19-23 13:05:[*] No.params is fixed: 788105\n",
      "07-19-23 13:05:[*] No.params is trainable: 207837\n",
      "07-19-23 13:05:[*] Start to calculate clusters. (totally GPU)\n",
      "07-19-23 13:08:[*] Get all representations.\n",
      "07-19-23 13:08:[*] attention shape: torch.Size([16911, 48, 63, 63])\n",
      "07-19-23 13:08:[*] t rep shape: torch.Size([16911, 48, 63, 32])\n",
      "07-19-23 13:08:[*] No. clusters: [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\n",
      "07-19-23 13:09:[*] Get all feature states.\n",
      "07-19-23 13:09:0 torch.Size([811728, 63])\n",
      "07-19-23 13:09:remove the missing code for origin feature\n",
      "07-19-23 13:09:0 torch.Size([362112, 63])\n",
      "07-19-23 13:09:[*] use all patients with this pattern to learn cohorts\n",
      "07-19-23 13:09:[*] Process Feature 0 with 7.901 seconds\n",
      "07-19-23 13:09:1 torch.Size([811728, 63])\n",
      "07-19-23 13:09:1 torch.Size([315984, 63])\n",
      "07-19-23 13:10:[*] Process Feature 1 with 13.762 seconds\n",
      "07-19-23 13:10:2 torch.Size([811728, 63])\n",
      "07-19-23 13:10:2 torch.Size([354144, 63])\n",
      "07-19-23 13:10:[*] Process Feature 2 with 11.097 seconds\n",
      "07-19-23 13:10:3 torch.Size([811728, 63])\n",
      "07-19-23 13:10:3 torch.Size([543120, 63])\n",
      "07-19-23 13:10:[*] Process Feature 3 with 19.383 seconds\n",
      "07-19-23 13:10:4 torch.Size([811728, 63])\n",
      "07-19-23 13:10:4 torch.Size([362064, 63])\n",
      "07-19-23 13:10:[*] Process Feature 4 with 4.509 seconds\n",
      "07-19-23 13:10:5 torch.Size([811728, 63])\n",
      "07-19-23 13:10:5 torch.Size([182160, 63])\n",
      "07-19-23 13:10:[*] Process Feature 5 with 5.090 seconds\n",
      "07-19-23 13:10:6 torch.Size([811728, 63])\n",
      "07-19-23 13:10:6 torch.Size([809424, 63])\n",
      "07-19-23 13:11:[*] Process Feature 6 with 30.276 seconds\n",
      "07-19-23 13:11:7 torch.Size([811728, 63])\n",
      "07-19-23 13:11:7 torch.Size([363648, 63])\n",
      "07-19-23 13:11:[*] Process Feature 7 with 12.378 seconds\n",
      "07-19-23 13:11:8 torch.Size([811728, 63])\n",
      "07-19-23 13:11:8 torch.Size([810384, 63])\n",
      "07-19-23 13:11:[*] Process Feature 8 with 27.259 seconds\n",
      "07-19-23 13:11:9 torch.Size([811728, 63])\n",
      "07-19-23 13:11:9 torch.Size([509808, 63])\n",
      "07-19-23 13:12:[*] Process Feature 9 with 13.304 seconds\n",
      "07-19-23 13:12:10 torch.Size([811728, 63])\n",
      "07-19-23 13:12:10 torch.Size([299328, 63])\n",
      "07-19-23 13:12:[*] Process Feature 10 with 5.083 seconds\n",
      "07-19-23 13:12:11 torch.Size([811728, 63])\n",
      "07-19-23 13:12:11 torch.Size([546336, 63])\n",
      "07-19-23 13:12:[*] Process Feature 11 with 18.539 seconds\n",
      "07-19-23 13:12:12 torch.Size([811728, 63])\n",
      "07-19-23 13:12:12 torch.Size([398064, 63])\n",
      "07-19-23 13:12:[*] Process Feature 12 with 9.606 seconds\n",
      "07-19-23 13:12:13 torch.Size([811728, 63])\n",
      "07-19-23 13:12:13 torch.Size([810384, 63])\n",
      "07-19-23 13:13:[*] Process Feature 13 with 20.293 seconds\n",
      "07-19-23 13:13:14 torch.Size([811728, 63])\n",
      "07-19-23 13:13:14 torch.Size([801696, 63])\n",
      "07-19-23 13:13:[*] Process Feature 14 with 18.818 seconds\n",
      "07-19-23 13:13:15 torch.Size([811728, 63])\n",
      "07-19-23 13:13:15 torch.Size([236208, 63])\n",
      "07-19-23 13:13:[*] Process Feature 15 with 7.764 seconds\n",
      "07-19-23 13:13:16 torch.Size([811728, 63])\n",
      "07-19-23 13:13:16 torch.Size([232176, 63])\n",
      "07-19-23 13:13:[*] Process Feature 16 with 3.828 seconds\n",
      "07-19-23 13:13:17 torch.Size([811728, 63])\n",
      "07-19-23 13:13:17 torch.Size([475728, 63])\n",
      "07-19-23 13:13:[*] Process Feature 17 with 19.785 seconds\n",
      "07-19-23 13:13:18 torch.Size([811728, 63])\n",
      "07-19-23 13:13:18 torch.Size([810768, 63])\n",
      "07-19-23 13:14:[*] Process Feature 18 with 23.504 seconds\n",
      "07-19-23 13:14:19 torch.Size([811728, 63])\n",
      "07-19-23 13:14:19 torch.Size([801696, 63])\n",
      "07-19-23 13:14:[*] Process Feature 19 with 32.353 seconds\n",
      "07-19-23 13:14:20 torch.Size([811728, 63])\n",
      "07-19-23 13:14:20 torch.Size([810336, 63])\n",
      "07-19-23 13:15:[*] Process Feature 20 with 32.529 seconds\n",
      "07-19-23 13:15:21 torch.Size([811728, 63])\n",
      "07-19-23 13:15:21 torch.Size([544080, 63])\n",
      "07-19-23 13:15:[*] Process Feature 21 with 11.203 seconds\n",
      "07-19-23 13:15:22 torch.Size([811728, 63])\n",
      "07-19-23 13:15:22 torch.Size([299616, 63])\n",
      "07-19-23 13:15:[*] Process Feature 22 with 12.607 seconds\n",
      "07-19-23 13:15:23 torch.Size([811728, 63])\n",
      "07-19-23 13:15:23 torch.Size([171504, 63])\n",
      "07-19-23 13:15:[*] Process Feature 23 with 7.933 seconds\n",
      "07-19-23 13:15:24 torch.Size([811728, 63])\n",
      "07-19-23 13:15:24 torch.Size([805824, 63])\n",
      "07-19-23 13:16:[*] Process Feature 24 with 23.768 seconds\n",
      "07-19-23 13:16:25 torch.Size([811728, 63])\n",
      "07-19-23 13:16:25 torch.Size([801600, 63])\n",
      "07-19-23 13:16:[*] Process Feature 25 with 20.771 seconds\n",
      "07-19-23 13:16:26 torch.Size([811728, 63])\n",
      "07-19-23 13:16:26 torch.Size([544800, 63])\n",
      "07-19-23 13:16:[*] Process Feature 26 with 13.427 seconds\n",
      "07-19-23 13:16:27 torch.Size([811728, 63])\n",
      "07-19-23 13:16:27 torch.Size([544848, 63])\n",
      "07-19-23 13:17:[*] Process Feature 27 with 17.455 seconds\n",
      "07-19-23 13:17:28 torch.Size([811728, 63])\n",
      "07-19-23 13:17:28 torch.Size([544800, 63])\n",
      "07-19-23 13:17:[*] Process Feature 28 with 16.896 seconds\n",
      "07-19-23 13:17:29 torch.Size([811728, 63])\n",
      "07-19-23 13:17:29 torch.Size([181152, 63])\n",
      "07-19-23 13:17:[*] Process Feature 29 with 6.336 seconds\n",
      "07-19-23 13:17:30 torch.Size([811728, 63])\n",
      "07-19-23 13:17:30 torch.Size([182160, 63])\n",
      "07-19-23 13:17:[*] Process Feature 30 with 3.321 seconds\n",
      "07-19-23 13:17:31 torch.Size([811728, 63])\n",
      "07-19-23 13:17:31 torch.Size([806208, 63])\n",
      "07-19-23 13:17:[*] Process Feature 31 with 18.782 seconds\n",
      "07-19-23 13:17:32 torch.Size([811728, 63])\n",
      "07-19-23 13:17:32 torch.Size([580320, 63])\n",
      "07-19-23 13:18:[*] Process Feature 32 with 21.620 seconds\n",
      "07-19-23 13:18:33 torch.Size([811728, 63])\n",
      "07-19-23 13:18:33 torch.Size([344688, 63])\n",
      "07-19-23 13:18:[*] Process Feature 33 with 7.584 seconds\n",
      "07-19-23 13:18:34 torch.Size([811728, 63])\n",
      "07-19-23 13:18:34 torch.Size([487200, 63])\n",
      "07-19-23 13:18:[*] Process Feature 34 with 23.832 seconds\n",
      "07-19-23 13:18:35 torch.Size([811728, 63])\n",
      "07-19-23 13:18:35 torch.Size([302640, 63])\n",
      "07-19-23 13:18:[*] Process Feature 35 with 9.017 seconds\n",
      "07-19-23 13:18:36 torch.Size([811728, 63])\n",
      "07-19-23 13:18:36 torch.Size([603744, 63])\n",
      "07-19-23 13:19:[*] Process Feature 36 with 8.922 seconds\n",
      "07-19-23 13:19:37 torch.Size([811728, 63])\n",
      "07-19-23 13:19:37 torch.Size([194352, 63])\n",
      "07-19-23 13:19:[*] Process Feature 37 with 8.250 seconds\n",
      "07-19-23 13:19:38 torch.Size([811728, 63])\n",
      "07-19-23 13:19:38 torch.Size([332352, 63])\n",
      "07-19-23 13:19:[*] Process Feature 38 with 8.890 seconds\n",
      "07-19-23 13:19:39 torch.Size([811728, 63])\n",
      "07-19-23 13:19:39 torch.Size([67056, 63])\n",
      "07-19-23 13:19:[*] Process Feature 39 with 2.812 seconds\n",
      "07-19-23 13:19:40 torch.Size([811728, 63])\n",
      "07-19-23 13:19:40 torch.Size([809952, 63])\n",
      "07-19-23 13:19:[*] Process Feature 40 with 14.781 seconds\n",
      "07-19-23 13:19:41 torch.Size([811728, 63])\n",
      "07-19-23 13:19:41 torch.Size([173712, 63])\n",
      "07-19-23 13:19:[*] Process Feature 41 with 9.088 seconds\n",
      "07-19-23 13:19:42 torch.Size([811728, 63])\n",
      "07-19-23 13:19:42 torch.Size([488544, 63])\n",
      "07-19-23 13:19:[*] Process Feature 42 with 9.843 seconds\n",
      "07-19-23 13:19:43 torch.Size([811728, 63])\n",
      "07-19-23 13:19:43 torch.Size([488496, 63])\n",
      "07-19-23 13:20:[*] Process Feature 43 with 9.891 seconds\n",
      "07-19-23 13:20:44 torch.Size([811728, 63])\n",
      "07-19-23 13:20:44 torch.Size([545280, 63])\n",
      "07-19-23 13:20:[*] Process Feature 44 with 17.598 seconds\n",
      "07-19-23 13:20:45 torch.Size([811728, 63])\n",
      "07-19-23 13:20:45 torch.Size([801456, 63])\n",
      "07-19-23 13:20:[*] Process Feature 45 with 28.990 seconds\n",
      "07-19-23 13:20:46 torch.Size([811728, 63])\n",
      "07-19-23 13:20:46 torch.Size([297120, 63])\n",
      "07-19-23 13:21:[*] Process Feature 46 with 11.342 seconds\n",
      "07-19-23 13:21:47 torch.Size([811728, 63])\n",
      "07-19-23 13:21:47 torch.Size([810288, 63])\n",
      "07-19-23 13:21:[*] Process Feature 47 with 27.518 seconds\n",
      "07-19-23 13:21:48 torch.Size([811728, 63])\n",
      "07-19-23 13:21:48 torch.Size([801696, 63])\n",
      "07-19-23 13:21:[*] Process Feature 48 with 27.251 seconds\n",
      "07-19-23 13:22:49 torch.Size([811728, 63])\n",
      "07-19-23 13:22:49 torch.Size([795312, 63])\n",
      "07-19-23 13:22:[*] Process Feature 49 with 23.500 seconds\n",
      "07-19-23 13:22:50 torch.Size([811728, 63])\n",
      "07-19-23 13:22:50 torch.Size([278064, 63])\n",
      "07-19-23 13:22:[*] Process Feature 50 with 5.827 seconds\n",
      "07-19-23 13:22:51 torch.Size([811728, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-19-23 13:22:51 torch.Size([592128, 63])\n",
      "07-19-23 13:22:[*] Process Feature 51 with 20.790 seconds\n",
      "07-19-23 13:22:52 torch.Size([811728, 63])\n",
      "07-19-23 13:22:52 torch.Size([809232, 63])\n",
      "07-19-23 13:23:[*] Process Feature 52 with 24.405 seconds\n",
      "07-19-23 13:23:53 torch.Size([811728, 63])\n",
      "07-19-23 13:23:53 torch.Size([233088, 63])\n",
      "07-19-23 13:23:[*] Process Feature 53 with 10.586 seconds\n",
      "07-19-23 13:23:54 torch.Size([811728, 63])\n",
      "07-19-23 13:23:54 torch.Size([46944, 63])\n",
      "07-19-23 13:23:[*] Process Feature 54 with 2.927 seconds\n",
      "07-19-23 13:23:55 torch.Size([811728, 63])\n",
      "07-19-23 13:23:55 torch.Size([118416, 63])\n",
      "07-19-23 13:23:[*] Process Feature 55 with 2.913 seconds\n",
      "07-19-23 13:23:56 torch.Size([811728, 63])\n",
      "07-19-23 13:23:56 torch.Size([95472, 63])\n",
      "07-19-23 13:23:[*] Process Feature 56 with 3.410 seconds\n",
      "07-19-23 13:23:57 torch.Size([811728, 63])\n",
      "07-19-23 13:23:57 torch.Size([88800, 63])\n",
      "07-19-23 13:23:[*] Process Feature 57 with 3.265 seconds\n",
      "07-19-23 13:23:58 torch.Size([811728, 63])\n",
      "07-19-23 13:23:58 torch.Size([507168, 63])\n",
      "07-19-23 13:23:[*] Process Feature 58 with 18.598 seconds\n",
      "07-19-23 13:23:59 torch.Size([811728, 63])\n",
      "07-19-23 13:23:59 torch.Size([124368, 63])\n",
      "07-19-23 13:24:[*] Process Feature 59 with 6.978 seconds\n",
      "07-19-23 13:24:60 torch.Size([811728, 63])\n",
      "07-19-23 13:24:60 torch.Size([279072, 63])\n",
      "07-19-23 13:24:[*] Process Feature 60 with 8.274 seconds\n",
      "07-19-23 13:24:61 torch.Size([811728, 63])\n",
      "07-19-23 13:24:61 torch.Size([295056, 63])\n",
      "07-19-23 13:24:[*] Process Feature 61 with 7.606 seconds\n",
      "07-19-23 13:24:62 torch.Size([811728, 63])\n",
      "07-19-23 13:24:62 torch.Size([245760, 63])\n",
      "07-19-23 13:24:[*] Process Feature 62 with 8.332 seconds\n",
      "07-19-23 13:24:[*] No.cohort in total\n",
      "07-19-23 13:24:[ 9579 26178 15629 28828  5758  7974 46197 20394 42509 19400  8088 28099\n",
      " 14370 34731 21360 12427  4148 39359 45487 61162 60463 13675 21057 14120\n",
      " 31299 28392 18221 26406 24241  8548  3124 23237 36960 12322 35526 15043\n",
      " 11229 13476 13437  4705 17754 23089 14963 12332 33465 52002 16110 37425\n",
      " 63259 42870  8745 35275 36763 16059  8186  4771  4680  7284 33891 12007\n",
      " 13605 10239 14121]\n",
      "07-19-23 13:24:[*] Filter1: No.cohort filtered by org freq\n",
      "07-19-23 13:24:[ 3382  6126  4842  8611  1780  2149 13431  5283 12047  5765  2085  8191\n",
      "  4113  8804  8223  3235  1542  8708 10265 14265 14261  4774  5473  3392\n",
      " 10391  8879  5683  7369  7217  2539  1269  8056  9301  3038 10293  3619\n",
      "  3630  3418  3747  1030  6346  3731  4055  3944  7504 12653  4695 11936\n",
      " 11769 10131  2324  9145 10548  4353   994   984  1264  1300  8047  2771\n",
      "  3313  3084  3268]\n",
      "07-19-23 13:24:[*] Filter2: No.cohort filtered by sample freq\n",
      "07-19-23 13:24:[ 3185  5762  4370  8117  1640  2006 12921  5036 11595  5247  1880  7653\n",
      "  4033  8587  8016  3083  1485  7986  9947 13911 13895  4469  5133  3030\n",
      "  9831  8698  5397  7067  6780  2291  1117  7863  9002  2941  9695  3366\n",
      "  3577  3280  3467   853  6035  3251  3972  3798  6903 12567  4279 11640\n",
      " 11318  9937  2125  8432  9772  4161   885   908  1168  1180  7727  2227\n",
      "  3034  2833  3088]\n",
      "07-19-23 13:24:[*] Filter3: No.cohort filtered by max size\n",
      "07-19-23 13:24:[]\n",
      "07-19-23 13:24:[*] minimal cohort freq\n",
      "07-19-23 13:24:[11. 11. 11. 12. 12. 12. 11. 13. 13. 11. 11. 11. 11. 11. 12. 12. 12. 12.\n",
      " 11. 14. 13. 12. 13. 14. 11. 11. 15. 11. 12. 11. 11. 13. 13. 12. 11. 11.\n",
      " 13. 15. 12. 11. 11. 13. 13. 11. 13. 12. 14. 11. 11. 11. 12. 11. 11. 15.\n",
      " 15. 12. 14. 12. 14. 14. 12. 14. 14.]\n",
      "07-19-23 13:24:[*] minimal cohort freq (sample-level)\n",
      "07-19-23 13:24:[5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.\n",
      " 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.\n",
      " 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
      "07-19-23 13:24:[*] Build all feature clusters: total time 1120.07s \n",
      "07-19-23 13:24:[*] Saving files...\n",
      "07-19-23 13:24:Train: [1][0/133]\tBT avg 6.018\tloss avg 0.2390\n",
      "07-19-23 13:26:Train: [1][20/133]\tBT avg 5.385\tloss avg 0.2673\n",
      "07-19-23 13:28:Train: [1][40/133]\tBT avg 5.300\tloss avg 0.2595\n",
      "07-19-23 13:29:Train: [1][60/133]\tBT avg 5.325\tloss avg 0.2528\n",
      "07-19-23 13:31:Train: [1][80/133]\tBT avg 5.290\tloss avg 0.2551\n",
      "07-19-23 13:33:Train: [1][100/133]\tBT avg 5.271\tloss avg 0.2530\n",
      "07-19-23 13:35:Train: [1][120/133]\tBT avg 5.264\tloss avg 0.2515\n",
      "07-19-23 13:38:Epoch 1, lr 0.000500, train loss 0.2532, train time 697.02s, valid time 165.97s, total time 862.99s.\n",
      "07-19-23 13:38:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2782   0.8655   0.5287   0.8832   0.4048   0.5071   \n",
      "test  0.2722   0.8652   0.5686   0.8907   0.4738   0.5305   \n",
      "\n",
      "07-19-23 13:38:[*] Saving files...\n",
      "07-19-23 13:38:Train: [2][0/133]\tBT avg 5.316\tloss avg 0.2373\n",
      "07-19-23 13:40:Train: [2][20/133]\tBT avg 5.214\tloss avg 0.2558\n",
      "07-19-23 13:42:Train: [2][40/133]\tBT avg 5.218\tloss avg 0.2487\n",
      "07-19-23 13:44:Train: [2][60/133]\tBT avg 5.198\tloss avg 0.2480\n",
      "07-19-23 13:45:Train: [2][80/133]\tBT avg 5.210\tloss avg 0.2479\n",
      "07-19-23 13:47:Train: [2][100/133]\tBT avg 5.242\tloss avg 0.2480\n",
      "07-19-23 13:49:Train: [2][120/133]\tBT avg 5.235\tloss avg 0.2526\n",
      "07-19-23 13:53:Epoch 2, lr 0.000499, train loss 0.2515, train time 693.19s, valid time 163.32s, total time 856.51s.\n",
      "07-19-23 13:53:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2786   0.8669   0.5294   0.8869   0.3980   0.5143   \n",
      "test  0.2717   0.8649   0.5684   0.8912   0.4471   0.5305   \n",
      "\n",
      "07-19-23 13:53:Train: [3][0/133]\tBT avg 4.996\tloss avg 0.2662\n",
      "07-19-23 13:55:Train: [3][20/133]\tBT avg 5.282\tloss avg 0.2600\n",
      "07-19-23 13:56:Train: [3][40/133]\tBT avg 5.230\tloss avg 0.2513\n",
      "07-19-23 13:58:Train: [3][60/133]\tBT avg 5.182\tloss avg 0.2544\n",
      "07-19-23 14:00:Train: [3][80/133]\tBT avg 5.159\tloss avg 0.2520\n",
      "07-19-23 14:01:Train: [3][100/133]\tBT avg 5.164\tloss avg 0.2513\n",
      "07-19-23 14:03:Train: [3][120/133]\tBT avg 5.177\tloss avg 0.2503\n",
      "07-19-23 14:07:Epoch 3, lr 0.000498, train loss 0.2529, train time 686.07s, valid time 159.95s, total time 846.02s.\n",
      "07-19-23 14:07:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2784   0.8658   0.5278   0.8860   0.4165   0.5053   \n",
      "test  0.2713   0.8658   0.5701   0.8926   0.4782   0.5338   \n",
      "\n",
      "07-19-23 14:07:Train: [4][0/133]\tBT avg 4.691\tloss avg 0.2241\n",
      "07-19-23 14:09:Train: [4][20/133]\tBT avg 5.142\tloss avg 0.2412\n",
      "07-19-23 14:10:Train: [4][40/133]\tBT avg 5.180\tloss avg 0.2507\n",
      "07-19-23 14:12:Train: [4][60/133]\tBT avg 5.181\tloss avg 0.2541\n",
      "07-19-23 14:14:Train: [4][80/133]\tBT avg 5.226\tloss avg 0.2502\n",
      "07-19-23 14:16:Train: [4][100/133]\tBT avg 5.216\tloss avg 0.2509\n",
      "07-19-23 14:17:Train: [4][120/133]\tBT avg 5.201\tloss avg 0.2511\n",
      "07-19-23 14:21:Epoch 4, lr 0.000497, train loss 0.2500, train time 689.63s, valid time 161.40s, total time 851.03s.\n",
      "07-19-23 14:21:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2785   0.8677   0.5286   0.8860   0.3929   0.5000   \n",
      "test  0.2714   0.8649   0.5691   0.8921   0.4545   0.5338   \n",
      "\n",
      "07-19-23 14:21:Train: [5][0/133]\tBT avg 5.571\tloss avg 0.1395\n",
      "07-19-23 14:23:Train: [5][20/133]\tBT avg 5.339\tloss avg 0.2384\n",
      "07-19-23 14:25:Train: [5][40/133]\tBT avg 5.229\tloss avg 0.2541\n",
      "07-19-23 14:26:Train: [5][60/133]\tBT avg 5.226\tloss avg 0.2493\n",
      "07-19-23 14:28:Train: [5][80/133]\tBT avg 5.226\tloss avg 0.2479\n",
      "07-19-23 14:30:Train: [5][100/133]\tBT avg 5.228\tloss avg 0.2499\n",
      "07-19-23 14:32:Train: [5][120/133]\tBT avg 5.248\tloss avg 0.2502\n",
      "07-19-23 14:35:Epoch 5, lr 0.000495, train loss 0.2495, train time 693.29s, valid time 164.13s, total time 857.42s.\n",
      "07-19-23 14:35:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2792   0.8669   0.5286   0.8865   0.3970   0.5071   \n",
      "test  0.2710   0.8656   0.5700   0.8921   0.4571   0.5269   \n",
      "\n",
      "07-19-23 14:35:Train: [6][0/133]\tBT avg 5.528\tloss avg 0.2483\n",
      "07-19-23 14:37:Train: [6][20/133]\tBT avg 5.232\tloss avg 0.2545\n",
      "07-19-23 14:39:Train: [6][40/133]\tBT avg 5.290\tloss avg 0.2480\n",
      "07-19-23 14:41:Train: [6][60/133]\tBT avg 5.309\tloss avg 0.2483\n",
      "07-19-23 14:42:Train: [6][80/133]\tBT avg 5.299\tloss avg 0.2489\n",
      "07-19-23 14:44:Train: [6][100/133]\tBT avg 5.309\tloss avg 0.2496\n",
      "07-19-23 14:46:Train: [6][120/133]\tBT avg 5.303\tloss avg 0.2503\n",
      "07-19-23 14:50:Epoch 6, lr 0.000493, train loss 0.2500, train time 702.31s, valid time 163.81s, total time 866.11s.\n",
      "07-19-23 14:50:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2774   0.8675   0.5273   0.8846   0.4106   0.4893   \n",
      "test  0.2714   0.8652   0.5715   0.8921   0.4818   0.5341   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-19-23 14:50:[*] Saving files...\n",
      "07-19-23 14:50:Train: [7][0/133]\tBT avg 5.018\tloss avg 0.2733\n",
      "07-19-23 14:52:Train: [7][20/133]\tBT avg 5.364\tloss avg 0.2532\n",
      "07-19-23 14:53:Train: [7][40/133]\tBT avg 5.305\tloss avg 0.2538\n",
      "07-19-23 14:55:Train: [7][60/133]\tBT avg 5.313\tloss avg 0.2511\n",
      "07-19-23 14:57:Train: [7][80/133]\tBT avg 5.359\tloss avg 0.2481\n",
      "07-19-23 14:59:Train: [7][100/133]\tBT avg 5.330\tloss avg 0.2495\n",
      "07-19-23 15:00:Train: [7][120/133]\tBT avg 5.324\tloss avg 0.2496\n",
      "07-19-23 15:04:Epoch 7, lr 0.000491, train loss 0.2494, train time 705.68s, valid time 166.34s, total time 872.03s.\n",
      "07-19-23 15:04:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2775   0.8675   0.5281   0.8836   0.4115   0.4858   \n",
      "test  0.2708   0.8662   0.5745   0.8903   0.4775   0.5429   \n",
      "\n",
      "07-19-23 15:04:Train: [8][0/133]\tBT avg 5.319\tloss avg 0.2395\n",
      "07-19-23 15:06:Train: [8][20/133]\tBT avg 5.565\tloss avg 0.2579\n",
      "07-19-23 15:08:Train: [8][40/133]\tBT avg 5.477\tloss avg 0.2521\n",
      "07-19-23 15:10:Train: [8][60/133]\tBT avg 5.454\tloss avg 0.2482\n",
      "07-19-23 15:12:Train: [8][80/133]\tBT avg 5.398\tloss avg 0.2447\n",
      "07-19-23 15:13:Train: [8][100/133]\tBT avg 5.370\tloss avg 0.2437\n",
      "07-19-23 15:15:Train: [8][120/133]\tBT avg 5.389\tloss avg 0.2484\n",
      "07-19-23 15:19:Epoch 8, lr 0.000488, train loss 0.2472, train time 714.31s, valid time 166.94s, total time 881.24s.\n",
      "07-19-23 15:19:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2783   0.8677   0.5288   0.8827   0.3861   0.5053   \n",
      "test  0.2705   0.8660   0.5736   0.8912   0.4651   0.5333   \n",
      "\n",
      "07-19-23 15:19:Train: [9][0/133]\tBT avg 5.200\tloss avg 0.2782\n",
      "07-19-23 15:21:Train: [9][20/133]\tBT avg 5.394\tloss avg 0.2408\n",
      "07-19-23 15:23:Train: [9][40/133]\tBT avg 5.325\tloss avg 0.2462\n",
      "07-19-23 15:24:Train: [9][60/133]\tBT avg 5.353\tloss avg 0.2385\n",
      "07-19-23 15:26:Train: [9][80/133]\tBT avg 5.346\tloss avg 0.2390\n",
      "07-19-23 15:28:Train: [9][100/133]\tBT avg 5.344\tloss avg 0.2412\n",
      "07-19-23 15:30:Train: [9][120/133]\tBT avg 5.368\tloss avg 0.2452\n",
      "07-19-23 15:34:Epoch 9, lr 0.000485, train loss 0.2457, train time 710.32s, valid time 166.66s, total time 876.98s.\n",
      "07-19-23 15:34:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2809   0.8681   0.5285   0.8813   0.3446   0.5036   \n",
      "test  0.2725   0.8653   0.5715   0.8936   0.4472   0.5305   \n",
      "\n",
      "07-19-23 15:34:Train: [10][0/133]\tBT avg 5.302\tloss avg 0.1885\n",
      "07-19-23 15:35:Train: [10][20/133]\tBT avg 5.420\tloss avg 0.2633\n",
      "07-19-23 15:37:Train: [10][40/133]\tBT avg 5.450\tloss avg 0.2508\n",
      "07-19-23 15:39:Train: [10][60/133]\tBT avg 5.420\tloss avg 0.2521\n",
      "07-19-23 15:41:Train: [10][80/133]\tBT avg 5.444\tloss avg 0.2476\n",
      "07-19-23 15:43:Train: [10][100/133]\tBT avg 5.447\tloss avg 0.2457\n",
      "07-19-23 15:44:Train: [10][120/133]\tBT avg 5.426\tloss avg 0.2464\n",
      "07-19-23 15:48:Epoch 10, lr 0.000481, train loss 0.2449, train time 718.57s, valid time 165.81s, total time 884.39s.\n",
      "07-19-23 15:48:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2776   0.8685   0.5291   0.8851   0.4116   0.5036   \n",
      "test  0.2709   0.8664   0.5758   0.8912   0.4773   0.5305   \n",
      "\n",
      "07-19-23 15:48:Train: [11][0/133]\tBT avg 4.765\tloss avg 0.2155\n",
      "07-19-23 15:50:Train: [11][20/133]\tBT avg 5.505\tloss avg 0.2427\n",
      "07-19-23 15:52:Train: [11][40/133]\tBT avg 5.383\tloss avg 0.2442\n",
      "07-19-23 15:54:Train: [11][60/133]\tBT avg 5.370\tloss avg 0.2469\n",
      "07-19-23 15:56:Train: [11][80/133]\tBT avg 5.374\tloss avg 0.2474\n",
      "07-19-23 15:57:Train: [11][100/133]\tBT avg 5.363\tloss avg 0.2447\n",
      "07-19-23 15:59:Train: [11][120/133]\tBT avg 5.391\tloss avg 0.2425\n",
      "07-19-23 16:02:Epoch 11, lr 0.000477, train loss 0.2434, train time 716.12s, valid time 133.93s, total time 850.05s.\n",
      "07-19-23 16:02:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2781   0.8686   0.5272   0.8836   0.4000   0.5000   \n",
      "test  0.2720   0.8651   0.5729   0.8898   0.4717   0.5341   \n",
      "\n",
      "07-19-23 16:02:Train: [12][0/133]\tBT avg 3.809\tloss avg 0.1705\n",
      "07-19-23 16:04:Train: [12][20/133]\tBT avg 3.970\tloss avg 0.2448\n",
      "07-19-23 16:05:Train: [12][40/133]\tBT avg 3.992\tloss avg 0.2478\n",
      "07-19-23 16:06:Train: [12][60/133]\tBT avg 3.974\tloss avg 0.2487\n",
      "07-19-23 16:08:Train: [12][80/133]\tBT avg 3.970\tloss avg 0.2428\n",
      "07-19-23 16:09:Train: [12][100/133]\tBT avg 3.948\tloss avg 0.2421\n",
      "07-19-23 16:10:Train: [12][120/133]\tBT avg 3.937\tloss avg 0.2425\n",
      "07-19-23 16:13:Epoch 12, lr 0.000473, train loss 0.2419, train time 521.81s, valid time 118.42s, total time 640.24s.\n",
      "07-19-23 16:13:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2793   0.8691   0.5271   0.8832   0.3810   0.5071   \n",
      "test  0.2723   0.8652   0.5727   0.8912   0.4626   0.5341   \n",
      "\n",
      "07-19-23 16:13:Train: [13][0/133]\tBT avg 3.803\tloss avg 0.2818\n",
      "07-19-23 16:14:Train: [13][20/133]\tBT avg 3.923\tloss avg 0.2459\n",
      "07-19-23 16:16:Train: [13][40/133]\tBT avg 3.935\tloss avg 0.2468\n",
      "07-19-23 16:17:Train: [13][60/133]\tBT avg 3.913\tloss avg 0.2487\n",
      "07-19-23 16:18:Train: [13][80/133]\tBT avg 3.899\tloss avg 0.2482\n",
      "07-19-23 16:20:Train: [13][100/133]\tBT avg 3.907\tloss avg 0.2428\n",
      "07-19-23 16:21:Train: [13][120/133]\tBT avg 3.901\tloss avg 0.2424\n",
      "07-19-23 16:24:Epoch 13, lr 0.000468, train loss 0.2404, train time 516.05s, valid time 117.78s, total time 633.83s.\n",
      "07-19-23 16:24:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2799   0.8670   0.5251   0.8827   0.4095   0.5036   \n",
      "test  0.2739   0.8652   0.5680   0.8903   0.4821   0.5305   \n",
      "\n",
      "07-19-23 16:24:Train: [14][0/133]\tBT avg 3.736\tloss avg 0.1846\n",
      "07-19-23 16:25:Train: [14][20/133]\tBT avg 3.681\tloss avg 0.2283\n",
      "07-19-23 16:26:Train: [14][40/133]\tBT avg 3.749\tloss avg 0.2319\n",
      "07-19-23 16:28:Train: [14][60/133]\tBT avg 3.800\tloss avg 0.2375\n",
      "07-19-23 16:29:Train: [14][80/133]\tBT avg 3.820\tloss avg 0.2374\n",
      "07-19-23 16:30:Train: [14][100/133]\tBT avg 3.836\tloss avg 0.2402\n",
      "07-19-23 16:34:Epoch 14, lr 0.000463, train loss 0.2382, train time 510.10s, valid time 119.02s, total time 629.13s.\n",
      "07-19-23 16:34:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2806   0.8695   0.5289   0.8827   0.3641   0.5124   \n",
      "test  0.2761   0.8622   0.5640   0.8921   0.4493   0.5161   \n",
      "\n",
      "07-19-23 16:34:Train: [15][0/133]\tBT avg 3.695\tloss avg 0.3268\n",
      "07-19-23 16:35:Train: [15][20/133]\tBT avg 3.810\tloss avg 0.2393\n",
      "07-19-23 16:37:Train: [15][40/133]\tBT avg 3.827\tloss avg 0.2389\n",
      "07-19-23 16:38:Train: [15][60/133]\tBT avg 3.852\tloss avg 0.2397\n",
      "07-19-23 16:41:Train: [15][100/133]\tBT avg 3.826\tloss avg 0.2343\n",
      "07-19-23 16:42:Train: [15][120/133]\tBT avg 3.852\tloss avg 0.2357\n",
      "07-19-23 16:45:Epoch 15, lr 0.000458, train loss 0.2376, train time 512.18s, valid time 119.51s, total time 631.69s.\n",
      "07-19-23 16:45:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2782   0.8691   0.5270   0.8851   0.4282   0.5000   \n",
      "test  0.2775   0.8614   0.5611   0.8879   0.4791   0.5269   \n",
      "\n",
      "07-19-23 16:45:Train: [16][0/133]\tBT avg 3.707\tloss avg 0.3157\n",
      "07-19-23 16:46:Train: [16][20/133]\tBT avg 3.883\tloss avg 0.2362\n",
      "07-19-23 16:47:Train: [16][40/133]\tBT avg 3.913\tloss avg 0.2421\n",
      "07-19-23 16:49:Train: [16][60/133]\tBT avg 3.920\tloss avg 0.2364\n",
      "07-19-23 16:50:Train: [16][80/133]\tBT avg 3.922\tloss avg 0.2348\n",
      "07-19-23 16:51:Train: [16][100/133]\tBT avg 3.938\tloss avg 0.2356\n",
      "07-19-23 16:53:Train: [16][120/133]\tBT avg 3.950\tloss avg 0.2363\n",
      "07-19-23 16:55:Epoch 16, lr 0.000452, train loss 0.2362, train time 524.07s, valid time 120.10s, total time 644.16s.\n",
      "07-19-23 16:55:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2792   0.8686   0.5233   0.8836   0.4087   0.5053   \n",
      "test  0.2767   0.8613   0.5627   0.8898   0.4764   0.5233   \n",
      "\n",
      "07-19-23 16:55:[*] Overfitting... Stop!\n",
      "07-19-23 16:55:=============== eval ===============\n",
      "07-19-23 16:55:[*] Cohorts loaded.\n",
      "07-19-23 16:55:[*] Model loaded!\n",
      "07-19-23 16:55:[*] skipped: 0  loaded: 1207\n",
      "07-19-23 16:57:\n",
      "      bceloss  auroc    auprc    accu     f1       minpse   \n",
      "valid 0.2774   0.8675   0.5273   0.8846   0.4106   0.4893   \n",
      "test  0.2714   0.8652   0.5715   0.8921   0.4818   0.5341   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CohortModelTrainer at 0x7f29a430e668>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# args.debug=True\n",
    "CohortModelTrainer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc7103",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}